{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b093224-a064-4ce3-a59c-6f0e7bbf6be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9e5588c5-ed9c-46e0-84ba-bdfc9d73b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(fileName):\n",
    "    '''Function to read the file and return the sentences in a list'''\n",
    "    \n",
    "    with open(fileName, 'r') as f:\n",
    "        sentences = [line.strip() for line in f.readlines()]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def build_vocabulary(tokens, min_frequency=3):\n",
    "    '''Function to Build the vocabulary, grouping rare tokens as '<unk>'''\n",
    "    vocab = []\n",
    "    vocab.append('<unk>')  # Add <unk> for rare words\n",
    "    vocab.append('<STOP>')  # Add <STOP> for sentence endings\n",
    "    \n",
    "    token_counts = collections.Counter(tokens)\n",
    "    # Include tokens with frequency >= min_frequency, plus '<unk>' and '<STOP>'\n",
    "    vocab.extend([token for token, count in token_counts.items() if count >= min_frequency])\n",
    "\n",
    "    return vocab, token_counts\n",
    "\n",
    "\n",
    "def preprocess_sentence(sentence, vocab):\n",
    "    '''Function to process the sentence to have unk and start and stop tags'''\n",
    "\n",
    "    words = sentence.split()\n",
    "    tokens = ['<START']\n",
    "    tokens.extend([word if word in vocab else '<unk>' for word in words])\n",
    "    tokens.append('<STOP>')  # Add <STOP> at the end of each sentence\n",
    "    return tokens\n",
    "        \n",
    "\n",
    "def create_ngrams(tokens, n):\n",
    "    '''Function to create N-grams'''\n",
    "    \n",
    "    ngrams = list(nltk.ngrams(tokens, n))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2a9ab53-040c-452b-b4a8-93fdb9aa8c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Having a little flexibility on that issue would go a long way to putting together a final package .',\n",
       " 'Long before the advent of e-commerce , Wal-Mart \\'s founder Sam Walton set out his vision for a successful retail operation : \" We let folks know we \\'re interested in them and that they \\'re vital to us-- \\' cause they are , \" he said .',\n",
       " 'A spokesman said the company has been affected by the credit crunch in the United States .']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the file to get the list of sentences\n",
    "sentences = readFile('1b_benchmark.train.tokens.txt')\n",
    "sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0fdfed7f-b0bc-4463-b2ef-ef4aa28149f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Having',\n",
       " 'a',\n",
       " 'little',\n",
       " 'flexibility',\n",
       " 'on',\n",
       " 'that',\n",
       " 'issue',\n",
       " 'would',\n",
       " 'go',\n",
       " 'a']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate the token from the sentences\n",
    "tokens = [word for sentence in sentences for word in sentence.split()]\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "76d5ed83-c4cf-4e9f-a2d1-1a0d5883030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the vocabulary\n",
    "vocab, token_counts = build_vocabulary(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1a73394f-e953-4465-a796-4969331bb55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of vocabulary is ::: 26602\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total length of vocabulary is ::: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "127a6825-bee7-401c-b156-b66af60c9815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<STOP>',\n",
       " 'Having',\n",
       " 'a',\n",
       " 'little',\n",
       " 'flexibility',\n",
       " 'on',\n",
       " 'that',\n",
       " 'issue',\n",
       " 'would']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "11f63c46-42c8-46ed-89d1-7e7c813d700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess each sentences\n",
    "process_sentences = [preprocess_sentence(sentence, vocab) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f8c78651-890f-430f-9c34-b2a759b182cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the list of processed sentences into a single list of tokens\n",
    "flattened_tokens = [token for sentence in process_sentences for token in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0212befd-76a6-4a40-81bf-2de4f2bf797d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<START',\n",
       " 'Having',\n",
       " 'a',\n",
       " 'little',\n",
       " 'flexibility',\n",
       " 'on',\n",
       " 'that',\n",
       " 'issue',\n",
       " 'would',\n",
       " 'go']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check two sentences of the processed lines\n",
    "flattened_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "91e5d7f0-de12-49b9-96b0-9191118dd139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create N-Grams\n",
    "unigram = create_ngrams(flattened_tokens, 1)\n",
    "bigram = create_ngrams(flattened_tokens, 2)\n",
    "trigram = create_ngrams(flattened_tokens, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "aca920ed-4b73-46af-ace7-a4f0448a7a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<START',), ('Having',), ('a',), ('little',), ('flexibility',)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "42960917-c2b7-431b-aa0b-884d7f465d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<START', 'Having'),\n",
       " ('Having', 'a'),\n",
       " ('a', 'little'),\n",
       " ('little', 'flexibility'),\n",
       " ('flexibility', 'on')]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0eb0ce8c-122a-4623-a54a-74766a3914fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<START', 'Having', 'a'),\n",
       " ('Having', 'a', 'little'),\n",
       " ('a', 'little', 'flexibility'),\n",
       " ('little', 'flexibility', 'on'),\n",
       " ('flexibility', 'on', 'that')]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255aa6e1-1dc9-4c41-89a1-61542d5344b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
